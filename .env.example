# CapyCut Configuration
# Copy this file to .env and fill in your values:
#   cp .env.example .env

# ===========================================
# LOCAL LLM Configuration (FREE - recommended!)
# ===========================================
# Works with LM Studio, Ollama, or any OpenAI-compatible server
# Used for BOTH video clipping AND image transcription!

# LM Studio (default port 1234):
# LLM_ENDPOINT=http://localhost:1234
# LLM_MODEL=your-model-name

# Ollama (default port 11434):
# LLM_ENDPOINT=http://localhost:11434
# LLM_MODEL=llama3.2

# For image transcription, use a vision-capable model:
# LLM_MODEL=llava         # LLaVA vision model
# LLM_MODEL=bakllava      # BakLLaVA
# LLM_MODEL=qwen-vl       # Qwen Vision Language

# Optional: Use different models for video vs image transcription
# IMAGE_LLM_ENDPOINT=http://localhost:1234
# IMAGE_LLM_MODEL=llava

# ===========================================
# Two-Stage Pipeline (Vision + Text Model)
# ===========================================
# For better results with local LLMs, you can use a two-stage pipeline:
# 1. Vision model (LLaVA, etc.) scans images and extracts raw text
# 2. Text model (Mistral, Llama, etc.) refines the text into polished markdown
#
# This is useful because vision models are good at OCR but may not produce
# the best formatted markdown, while text models excel at text processing.

# IMAGE_VISION_MODEL=llava           # Vision model for image scanning
# IMAGE_TEXT_MODEL=mistral           # Text/agentic model for refinement
# IMAGE_TEXT_ENDPOINT=http://localhost:1234  # Optional: separate endpoint for text model

# ===========================================
# Azure OpenAI (Alternative for video clipping)
# ===========================================
# Get these from your Azure OpenAI resource in the Azure Portal

# AZURE_OPENAI_ENDPOINT=https://your-resource.cognitiveservices.azure.com
# AZURE_OPENAI_API_KEY=your-api-key-here
# AZURE_OPENAI_MODEL=gpt-4o
# AZURE_OPENAI_API_VERSION=2025-04-01-preview

# ===========================================
# Azure Anthropic (Claude) - Image transcription
# ===========================================
# Deploy Claude models via Azure AI Studio / Model Catalog
# Supports Claude Sonnet 4, Claude 3.5 Sonnet, etc.

# AZURE_ANTHROPIC_ENDPOINT=https://your-resource.services.ai.azure.com
# AZURE_ANTHROPIC_API_KEY=your-api-key-here
# AZURE_ANTHROPIC_MODEL=claude-sonnet-4-20250514  # Optional, defaults to claude-sonnet-4-20250514

# ===========================================
# Google Gemini API (Alternative for image transcription)
# ===========================================
# Get your API key from: https://aistudio.google.com/apikey

# GEMINI_API_KEY=your-gemini-api-key-here
# or
# GOOGLE_API_KEY=your-google-api-key-here
